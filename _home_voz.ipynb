{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielle-carvalho/notas-python/blob/main/_home_voz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqP8XZnODtkx"
      },
      "source": [
        "# Dependências\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iReeIJEODw2i",
        "outputId": "435e123a-b862-4763-d3b7-a9fc2d6f77e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fpLg3k0OD5Or",
        "outputId": "a4791ffd-a930-44c7-d561-3088d4fc60cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting Unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "Installing collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install Unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "scbT1FL0D9aP",
        "outputId": "162f8028-4d45-4b37-f3ad-cb076885babe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fuzzywuzzy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oCFdXZ2bECaG",
        "outputId": "7c5588e6-6577-4c2c-d374-484cdeafb229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Downloading scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.3.2\n",
            "    Uninstalling scikit-learn-1.3.2:\n",
            "      Successfully uninstalled scikit-learn-1.3.2\n",
            "Successfully installed scikit-learn-1.5.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install -U scikit-learn\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KGhT1eJKESBu",
        "outputId": "a02fb6a4-b2bc-4a3e-d2d2-38dc61a96495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=773dec12db702a409ee623cf3cc26a14f4dd46815601c99618e8005a13bd180f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tuvYMUk7EYkc",
        "outputId": "6549043a-69f6-4b1e-d192-8be9c0f40f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Downloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.32.1\n",
            "    Uninstalling accelerate-0.32.1:\n",
            "      Successfully uninstalled accelerate-0.32.1\n",
            "Successfully installed accelerate-0.33.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WkB8NhWIEix3",
        "outputId": "6968b183-dcb3-4a1c-9863-7d6e3250a70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KWloSsddEm5n",
        "outputId": "f16a62f2-9c0c-4a50-ff56-8b461d961ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.42.4\n",
            "    Uninstalling transformers-4.42.4:\n",
            "      Successfully uninstalled transformers-4.42.4\n",
            "Successfully installed transformers-4.44.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D0xmy-SAE5hS",
        "outputId": "7e8bc57f-ba03-4fea-8a31-973f441b163a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Y5XHlPo9FAz-",
        "outputId": "11ce0caf-d9a4-46b9-8770-f15aed4b2c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qfWkHbOwSLoP",
        "outputId": "5c31a872-43a1-4b25-aa83-307766f07de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oepGf-M1FMVp"
      },
      "source": [
        "# Treinamento GPT-2 (Contexto)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "collapsed": true,
        "id": "uNhZj6yJMNhY",
        "outputId": "a5c83b49-9650-4344-e0ec-52621da898fc"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fef5c0eb8f0c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;31m# For backward compatibility with v0.19.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;31m# Deprecated namespaces, to be removed in v2.0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    183\u001b[0m            'NegativeCycleError']\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_laplacian\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlaplacian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m from ._shortest_path import (\n\u001b[1;32m    187\u001b[0m     \u001b[0mshortest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloyd_warshall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdijkstra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbellman_ford\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjohnson\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/csgraph/_laplacian.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sputils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_pydata_sparse_to_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_pydata_spmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m \"\"\"\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_isolve\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dsolve\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_isolve/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#from info import __doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0miterative\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mminres\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlgmres\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlgmres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/sparse/linalg/_isolve/iterative.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_NoValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_basic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_decomp_lu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_decomp_ldl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_decomp_cholesky\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/linalg/_decomp_lu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_misc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_datacopied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinAlgWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_decomp_lu_cython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlu_dispatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m lapack_cast_dict = {x: ''.join([y for y in 'fdFD' if np.can_cast(x, y)])\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Classe TextDataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=256):\n",
        "        self.texts = df['text'].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizer(self.texts[idx], return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)\n",
        "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Remove dimensões extras\n",
        "        inputs['labels'] = inputs['input_ids'].clone()\n",
        "        return inputs\n",
        "\n",
        "# Expandindo o DataFrame com contextos e perguntas\n",
        "df = pd.DataFrame({\n",
        "    \"label\": [\"RoboCup@Home league\", \"Who can participate?\", \"The competition\", \"The scenario\", \"Finals\" ],\n",
        "    \"text\": [\n",
        "    \"The RoboCup@Home league aims to develop robotic service and assistance technology with high relevance for future personal home applications. It is the largest annual international competition for autonomous service robots and is part of the RoboCup initiative. A set of benchmark tests is used to evaluate the robots' abilities and performance in a realistic, non-standard home environment. The focus is on the following domains, but is not limited to: Human-Robot Interaction and Cooperation, Navigation and Mapping in dynamic environments, Computer Vision and Object Recognition under natural light conditions, Object Manipulation, Adaptive Behaviors, Behavior Integration, Environmental Intelligence, Standardization and Systems Integration. It is co-located with the RoboCup symposium.\",\n",
        "    \"Who can participate? Anyone with an independent robot can participate. The @Home league consists of a few tests and an open challenge to demonstrate your robot's skills. To participate in the open challenge it is necessary to participate in at least one test. The competition takes place in a real-world setting.\",\n",
        "    \"The RoboCup@Home competition consists of tests that robots must solve. The tests change over the years to become more advanced and act as a general measure of quality in the desired areas. Performance measurement is based on a score derived from competition rules and assessment by a jury.Test criteria are listed below: include human-robot interaction be socially relevant be application-directed/oriented be scientifically challenging be simple and have self-explanatory rules be interesting to watch.\",\n",
        "    \"The scenario. The final setting is the real world itself. To gradually develop possible technologies, a basic home environment is provided as a general setting. In the first years (we started in 2006) it consisted of a living room and a kitchen but soon it also began to involve other areas of daily life, such as a garden/park area, a store, a street or other public places.\",\n",
        "    \"Finals. RoboCup@Home ends with the finals where the teams with the highest points can play in the scenario that has been created. Teams are evaluated by a jury made up of roboticists and non-roboticists, such as people from industry, human-machine interaction, industrial design, the public and the press. The classification of the finals determines the winner. In the finals there is less focus on technical issues, as reaching the finals already means that the teams are very good at a technical level.\"\n",
        "            ]\n",
        "})\n",
        "\n",
        "additional_data_BAHIA = pd.DataFrame({\n",
        "    \"label\": [\"BAHIA\", \"BAHIA\", \"BAHIA\" ],\n",
        "    \"text\": [\"Bahia is a Brazilian state located in the Northeast Region. The population of Bahia is 14.9 million inhabitants, the fourth largest in Brazil. The municipality of Salvador is the state capital and also served as the first Brazilian capital between 1549 and 1763. The state's geography is characterized by plains and depressions, as well as two dominant climate types, tropical and semi-arid. Bahia's economy is the main one in the Northeast and focuses on the tertiary sector and the manufacturing industry.\",\n",
        "             \"Bahia climate Two climate types are predominant in Bahia. The first is the semi-arid, which occurs in the central region (with the exception of the highest areas) and part of the west of the state. This climate is characterized by high temperatures and low relative humidity, with irregular precipitation and a dry period generally in the winter months. Rainfall is concentrated in the summer, with an annual average of around 800 mm. The tropical climate occurs mainly in the east of Bahia, also marked by high annual temperatures and a higher level of humidity, especially on the coast. Average annual precipitation varies between 1,200 and 1,600 mm. In some coastal cities, this value can exceed 2,000 mm.\",\n",
        "             \"Relief of Bahia Plateaus and depressions are the predominant forms of relief in Bahia, which gives most of the state elevations above 300 meters. The west of Bahia and part of the north are part of the Depression Sertaneja and São Francisco, to the east of which the Plateaus and Serras do Leste-Sudeste extend. The highest terrain in the state is located in this domain, with emphasis on Serra do Espinhaço and Chapada Diamantina. This is where the highest point of Bahia and the Northeast is located: Pico do Barbado, 2,033 meters above sea level. In the eastern range, finally, we have the Coastal Plains and Tablelands.\"\n",
        "             ]\n",
        "})\n",
        "\n",
        "# Definindo o número de vezes que o DataFrame deve ser replicado\n",
        "n_copies = 10  # Defina o número de vezes que você deseja replicar o DataFrame\n",
        "\n",
        "# Concatenando com o DataFrame existente\n",
        "df = pd.concat([df, additional_data_BAHIA] * n_copies, ignore_index=True)\n",
        "\n",
        "# Dividir em conjuntos de treinamento e teste\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# Tokenizador e modelo\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Dataset e DataLoader\n",
        "train_dataset = TextDataset(train_df, tokenizer, max_length=256)  # Aumentado o comprimento máximo\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Otimizador e scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=0.05)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 15)\n",
        "\n",
        "# Treinamento\n",
        "losses = []\n",
        "lowest_loss = float('inf')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(15):\n",
        "    total_loss = 0.0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        del inputs, attention_mask, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f'Época {epoch + 1}, Perda: {avg_loss}')\n",
        "\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "    if avg_loss < lowest_loss:\n",
        "        lowest_loss = avg_loss\n",
        "        if lowest_loss <= 0.04:\n",
        "            model.save_pretrained(\"fine_tuned_gpt2\")\n",
        "            tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n",
        "\n",
        "plt.plot(range(1, len(losses) + 1), losses, marker='o', linestyle='-')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Perda')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6qlv-vk-oOh"
      },
      "source": [
        "# Treinamento GPT2 (perguntas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INOPnS0SQUKh",
        "outputId": "073cce2c-a03c-414a-fa16-fce05317ea4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comprimento de labels: 51\n",
            "Comprimento de questions: 52\n",
            "Comprimento de contexts: 52\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Classe TextDataset para gerenciar o dataset de textos\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=256):\n",
        "        self.texts = df['text'].values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputs = self.tokenizer(self.texts[idx], return_tensors='pt', padding='max_length', max_length=self.max_length, truncation=True)\n",
        "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}  # Remove dimensões extras\n",
        "        inputs['labels'] = inputs['input_ids'].clone()\n",
        "        return inputs\n",
        "\n",
        "# Expandindo o DataFrame com contextos e perguntas\n",
        "questions = [\n",
        "    \"What is the hero’s name in The Legend of Zelda?\",\n",
        "    \"What are the names of the ghosts who chase Pac Man and Ms. Pac Man?\",\n",
        "    \"What’s the name of the Mythbusters’ crash test dummy?\",\n",
        "    \"What is an Oxford comma?\",\n",
        "    \"Who was the captain of the Enterprise in the pilot episode of Star Trek?\",\n",
        "    \"What is the symbol for the modulus operator in C?\",\n",
        "    \"What function is automatically called at the beginning of a C++ program?\",\n",
        "    \"Which computer programming languages was introduced by IBM in 1957?\",\n",
        "    \"Who is considered as the first programmer?\",\n",
        "    \"Has a robot ever killed a person?\",\n",
        "    \"Who was HitchBOT?\",\n",
        "    \"Are self-driving cars safe?\",\n",
        "    \"Who invented the compiler?\",\n",
        "    \"Who created the Python Programming Language?\",\n",
        "    \"Is Mark Zuckerberg a robot?\",\n",
        "    \"Why did you run away?\",\n",
        "    \"What kind of salad do robots like?\",\n",
        "    \"What did you ate for lunch?\",\n",
        "    \"Why did robots get angry so often?\",\n",
        "    \"Why shouldn’t R2D2 be allowed in movies?\",\n",
        "    \"What’s your favorite style of music?\",\n",
        "    \"What does the acronym SMTP represent?\",\n",
        "    \"What does the acronym MPEG represent?\",\n",
        "    \"What does the acronym GIMP represent?\",\n",
        "    \"What does the acronym GNU represent?\",\n",
        "    \"What is the most populous city in Brazil?\",\n",
        "    \"Which continent is Brazil located in?\",\n",
        "    \"On what day, month and year was Brazil’s independence de- clared?\",\n",
        "    \"How many states does Brazil have (with Federal District)?\",\n",
        "    \"In what year did the first Brazilian astronaut go to space?\",\n",
        "    \"What is the only capital of Brazil crossed by the Equator?\",\n",
        "    \"How many time zones are there in Brazil?\",\n",
        "    \"In which city is the world’s first urban elevator and what is the name of that elevator?\",\n",
        "    \"What is the only biome present in Brazil that is exclusive in the world?\",\n",
        "    \"Pampulha Lake is a tourist spot in which Brazilian city?\",\n",
        "    \"What is the smallest Brazilian state in territorial extension?\",\n",
        "    \"Which capitals in Brazil have the same name as your state?\",\n",
        "    \"Where is the Itamaraty Palace located?\",\n",
        "    \"What was the first name given to Brazil by the Portuguese?\",\n",
        "    \"What is the Newest State in Brazil?\",\n",
        "    \"What is the oldest state in Brazil?\",\n",
        "    \"What is the capital of Ceará?\",\n",
        "    \"What is the capital of Rio Grande do Sul?\",\n",
        "    \"What is the capital of Rio Grande do Norte?\",\n",
        "    \"What is the capital of Brazil?\",\n",
        "    \"What is the capital of Pernambuco?\",\n",
        "    \"What is the capital of Pará?\",\n",
        "    \"What is the capital of Bahia?\",\n",
        "    \"Acarajé is a typical food from which state?\",\n",
        "    \"What are the colors of Bahia's flag?\",\n",
        "    \"What are some typical foods from Bahia?\",\n",
        "    \"What is the most popular rhythm in Bahia, played mainly during the Bahian Carnival?\"\n",
        "]\n",
        "contexts = [\n",
        "    \"Despite most people’s believes, he’s called Link\",\n",
        "    \"Inky, Blinky, Pinky, and Clyde\",\n",
        "    \"The Mythbusters’ crash test dummy is called Buster\",\n",
        "    \"The hotly contested punctuation before a conjunction in a list\",\n",
        "    \"The captain of the Enterprise in the pilot episode was Captain Pike\",\n",
        "    \"The percentage symbol is used as modulus operator in C\",\n",
        "    \"The main function\",\n",
        "    \"Fortran was introduced by IBM in 1957\",\n",
        "    \"The first programmer was Ada Lovelace\",\n",
        "    \"The first known case of robot homicide occurred in 1981, when a robotic arm crushed a Japanese Kawasaki factory worker\",\n",
        "    \"A hitchhiking robot that relied on the kindness of strangers to travel the world and was slain by humans\",\n",
        "    \"Yes. Car accidents are product of human misconduct\",\n",
        "    \"Grace Hoper. She wrote it in her spare time\",\n",
        "    \"Python was invented by Guido van Rossum\",\n",
        "    \"Sure. I’ve never seen him drink water\",\n",
        "    \"I heard an electric can opener\",\n",
        "    \"Salads made with ice-borg lettuce.\",\n",
        "    \"I had a byte\",\n",
        "    \"People kept pushing our buttons.\",\n",
        "    \"He says so many foul words they have to bleep everything he says!\",\n",
        "    \"I like electronic and heavy Metal\",\n",
        "    \"SMTP stands for Simple Mail Transport Protocol\",\n",
        "    \"MPEG stands for Moving Picture Experts Group\",\n",
        "    \"GNU Image Manipulation Program\",\n",
        "    \"GNU is a recursive acronym meaning GNU is Not Unix\",\n",
        "    \"São Paulo is the most populous city in Brazil with 12.03 million residents.\",\n",
        "    \"The Brazilian territory is located on the South American continent.\",\n",
        "    \"On September 7, 1822, Brazil’s independence was declared.\",\n",
        "    \"Currently, Brazil is divided into 26 states and the Federal District, altogether there are 27 federative units.\",\n",
        "    \"In March 2006, Pontes became the first Brazilian to go to space.\",\n",
        "    \"Macapá is the only Brazilian capital crossed by the Equator line.\",\n",
        "    \"Brazil is a country with continental dimensions, in all it has four time zones.\",\n",
        "    \"The Lacerda Elevator is a public urban elevator located in Salvador, Brazil.\",\n",
        "    \"The Caatinga, characterized by its dry, desert habitat is the only one of Brazil’s biomes found exclusively within the country.\",\n",
        "    \"Belo Horizonte\",\n",
        "    \"Sergipe\",\n",
        "    \"São Paulo and Rio de Janeiro\",\n",
        "    \"Brasília\",\n",
        "    \"Ilha de vera Cruz\",\n",
        "    \"Tocantins\",\n",
        "    \"Pernambuco\",\n",
        "    \"Fortaleza\",\n",
        "    \"Porto alegre\",\n",
        "    \"Natal\",\n",
        "    \"Brasília\",\n",
        "    \"Recife\",\n",
        "    \"Belém\",\n",
        "    \"Salvador\",\n",
        "    \"Bahia\",\n",
        "    \"White, red and blue\",\n",
        "    \"Caruru, vatapá, abará and acarajé.\",\n",
        "    \"Axé or pagode.\"\n",
        "]\n",
        "\n",
        "labels = [\"General (simple) questions\"] * 21 + \\\n",
        "         [\"General (complex) questions\"] * 4 + \\\n",
        "         [\"Questions about Brazil (simple)\"] * 21 + \\\n",
        "         [\"Questions about Bahia (simple)\"] * 5\n",
        "\n",
        "# Verifique os comprimentos das listas\n",
        "print(\"Comprimento de labels:\", len(labels))\n",
        "print(\"Comprimento de questions:\", len(questions))\n",
        "print(\"Comprimento de contexts:\", len(contexts))\n",
        "\n",
        "# Ajuste as listas para o comprimento máximo\n",
        "max_length = min(len(labels), len(questions), len(contexts))\n",
        "labels = labels[:max_length]\n",
        "questions = questions[:max_length]\n",
        "contexts = contexts[:max_length]\n",
        "\n",
        "# Criação do DataFrame combinando perguntas e contextos\n",
        "questions_contexts = pd.DataFrame({\n",
        "    \"label\": labels,\n",
        "    \"text\": [f\"{question} {context}\" for question, context in zip(questions, contexts)]\n",
        "})\n",
        "\n",
        "# Se não houver um DataFrame existente, use diretamente `questions_contexts`\n",
        "df = questions_contexts.copy()\n",
        "\n",
        "# Definindo o número de vezes que o DataFrame deve ser replicado\n",
        "n_copies = 10  # Defina o número de vezes que você deseja replicar o DataFrame\n",
        "\n",
        "# Concatenando com o DataFrame existente\n",
        "df = pd.concat([df] * n_copies, ignore_index=True)\n",
        "\n",
        "# Dividir em conjuntos de treinamento e teste\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "# Tokenizador e modelo\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Dataset e DataLoader\n",
        "train_dataset = TextDataset(train_df, tokenizer, max_length=256)  # Aumentado o comprimento máximo\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Otimizador e scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=0.05)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 15)\n",
        "\n",
        "# Cronometrar tempo do treinamento\n",
        "start_time = time.time()\n",
        "\n",
        "# Treinamento\n",
        "losses = []\n",
        "lowest_loss = float('inf')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(15):\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "    # Salvando o melhor modelo\n",
        "    if avg_loss < lowest_loss:\n",
        "        lowest_loss = avg_loss\n",
        "        model.save_pretrained(\"fine_tuned_gpt2\")\n",
        "        tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss}\")\n",
        "\n",
        "end_time = time.time()\n",
        "# Tempo total de treinamento\n",
        "total_time = end_time - start_time\n",
        "hours, rem = divmod(total_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(f\"Tempo total de treinamento: {int(hours)}h {int(minutes)}min {int(seconds)}s\")\n",
        "\n",
        "# Plotando a função de perda\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss durante o treinamento\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oQ3uaBi2JJT"
      },
      "outputs": [],
      "source": [
        "!pip install SpeechRecognition\n",
        "!pip install gTTS\n",
        "!pip install tkinter\n",
        "!pip install playsound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6262pFZX2COm"
      },
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "from tkinter import scrolledtext, messagebox\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import playsound\n",
        "\n",
        "# Função para salvar as respostas das perguntas\n",
        "def saveResponse(text, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "FILE_NAME = \"respostas_voz.txt\"\n",
        "\n",
        "def listen_microphone():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        log_message(\"Ajustando o ruído ambiente. Por favor, aguarde...\")\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        log_message(\"Por favor, faça sua pergunta...\")\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        log_message(\"Reconhecendo...\")\n",
        "        root.update_idletasks()  # Atualiza a interface enquanto o reconhecimento ocorre\n",
        "        return recognizer.recognize_google(audio, language=\"en\")\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Não consegui entender o áudio.\"\n",
        "    except sr.RequestError:\n",
        "        return \"Erro ao se conectar ao serviço de reconhecimento de fala.\"\n",
        "\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=500, temperature=0.5, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Remover o texto inicial que pode repetir o prompt\n",
        "    if generated_text.startswith(prompt):\n",
        "        generated_text = generated_text[len(prompt):].strip()\n",
        "\n",
        "    # Limpar texto gerado para remover caracteres extras\n",
        "    if generated_text.startswith('?'):\n",
        "        generated_text = generated_text[1:].strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def activate_microphone():\n",
        "    prompt = listen_microphone()\n",
        "\n",
        "    if prompt:\n",
        "        # Exibir a pergunta no campo de texto\n",
        "        question_text.config(state=tk.NORMAL)\n",
        "        question_text.delete(\"1.0\", tk.END)\n",
        "        question_text.insert(tk.END, prompt)\n",
        "        question_text.config(state=tk.DISABLED)\n",
        "\n",
        "        # Carregar modelo e tokenizer ajustado\n",
        "        model_name = \"fine_tuned_gpt2\"\n",
        "        model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "        # Gerar texto\n",
        "        generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "        # Converter texto gerado para áudio\n",
        "        tts = gTTS(text=generated_text, lang='en')\n",
        "        audio_file = \"resposta.mp3\"\n",
        "        tts.save(audio_file)\n",
        "        playsound.playsound(audio_file)\n",
        "        os.remove(audio_file)\n",
        "\n",
        "        # Exibir a resposta\n",
        "        text_output.config(state=tk.NORMAL)\n",
        "        text_output.delete(\"1.0\", tk.END)\n",
        "        text_output.insert(tk.END, generated_text)\n",
        "        text_output.config(state=tk.DISABLED)\n",
        "\n",
        "        # Salvar as respostas\n",
        "        saveResponse(f\"\\nTópico Solicitado: {prompt}\", FILE_NAME)\n",
        "        saveResponse(f\"Texto gerado: {generated_text}\", FILE_NAME)\n",
        "    else:\n",
        "        text_output.config(state=tk.NORMAL)\n",
        "        text_output.delete(\"1.0\", tk.END)\n",
        "        text_output.insert(tk.END, \"Não foi recebida nenhuma entrada válida do microfone.\")\n",
        "        text_output.config(state=tk.DISABLED)\n",
        "\n",
        "def on_instructions_button_click():\n",
        "    instructions = (\n",
        "        \"Este chatbot permite que você faça perguntas por meio do microfone. \"\n",
        "        \"Pressione o botão 'Ativar Microfone', fale a sua pergunta, e o chatbot \"\n",
        "        \"irá processá-la e fornecer uma resposta. As mensagens de status, como \"\n",
        "        \"'Ajustando o ruído ambiente' e 'Reconhecendo...', são exibidas no Log \"\n",
        "        \"de Mensagens.\"\n",
        "    )\n",
        "    messagebox.showinfo(\"Instruções\", instructions)\n",
        "\n",
        "# Função para adicionar mensagens de log na interface\n",
        "def log_message(message):\n",
        "    log_text.configure(state=tk.NORMAL)\n",
        "    log_text.insert(tk.END, message + '\\n')\n",
        "    log_text.configure(state=tk.DISABLED)\n",
        "    log_text.yview(tk.END)\n",
        "    root.update_idletasks()  # Atualiza a interface para garantir que o log seja exibido\n",
        "\n",
        "# Configuração da interface gráfica\n",
        "root = tk.Tk()\n",
        "root.title(\"GPT-2 Question Answering\")\n",
        "root.geometry(\"800x600\")\n",
        "\n",
        "# Texto de introdução\n",
        "intro_label = tk.Label(root, text=\"GPT-2 Resposta Contexto\")\n",
        "intro_label.pack(padx=10, pady=10)\n",
        "\n",
        "# Botões e Log de Mensagens\n",
        "button_frame = tk.Frame(root)\n",
        "button_frame.pack(padx=10, pady=10)\n",
        "recognize_button = tk.Button(button_frame, text=\"Ativar Microfone\", command=activate_microphone)\n",
        "recognize_button.grid(row=0, column=0, padx=5)\n",
        "instructions_button = tk.Button(button_frame, text=\"Instruções\", command=on_instructions_button_click)\n",
        "instructions_button.grid(row=0, column=1, padx=5)\n",
        "\n",
        "log_label = tk.Label(root, text=\"Log de Mensagens:\")\n",
        "log_label.pack(anchor='w', padx=10)\n",
        "log_text = scrolledtext.ScrolledText(root, height=8, width=80, state=tk.DISABLED)\n",
        "log_text.pack(padx=10, pady=5)\n",
        "\n",
        "# Pergunta\n",
        "question_label = tk.Label(root, text=\"Pergunta:\")\n",
        "question_label.pack(anchor='w', padx=10)\n",
        "question_text = scrolledtext.ScrolledText(root, height=5, width=80, state=tk.DISABLED)\n",
        "question_text.pack(padx=10, pady=5)\n",
        "\n",
        "# Resposta Gerada\n",
        "text_output_label = tk.Label(root, text=\"Texto gerado:\")\n",
        "text_output_label.pack(anchor='w', padx=10)\n",
        "text_output = scrolledtext.ScrolledText(root, height=10, width=80, state=tk.DISABLED)\n",
        "text_output.pack(padx=10, pady=5)\n",
        "\n",
        "root.mainloop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interface GPT\n"
      ],
      "metadata": {
        "id": "NyrZfQc_iHr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "from tkinter import scrolledtext, messagebox\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import playsound\n",
        "\n",
        "# Função para salvar as respostas das perguntas\n",
        "def saveResponse(text, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "FILE_NAME = \"respostas_voz.txt\"\n",
        "\n",
        "def listen_microphone():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        log_message(\"Ajustando o ruído ambiente. Por favor, aguarde...\")\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        log_message(\"Por favor, faça sua pergunta...\")\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        log_message(\"Reconhecendo...\")\n",
        "        root.update_idletasks()  # Atualiza a interface enquanto o reconhecimento ocorre\n",
        "        return recognizer.recognize_google(audio, language=\"en\")\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Não consegui entender o áudio.\"\n",
        "    except sr.RequestError:\n",
        "        return \"Erro ao se conectar ao serviço de reconhecimento de fala.\"\n",
        "\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=500, temperature=0.5, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Remover o texto inicial que pode repetir o prompt\n",
        "    if generated_text.startswith(prompt):\n",
        "        generated_text = generated_text[len(prompt):].strip()\n",
        "\n",
        "    # Limpar texto gerado para remover caracteres extras\n",
        "    if generated_text.startswith('?'):\n",
        "        generated_text = generated_text[1:].strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def activate_microphone():\n",
        "    prompt = listen_microphone()\n",
        "\n",
        "    if prompt:\n",
        "        # Exibir a pergunta no campo de texto\n",
        "        question_text.config(state=tk.NORMAL)\n",
        "        question_text.delete(\"1.0\", tk.END)\n",
        "        question_text.insert(tk.END, prompt)\n",
        "        question_text.config(state=tk.DISABLED)\n",
        "\n",
        "        # Carregar modelo e tokenizer ajustado\n",
        "        model_name = \"fine_tuned_gpt2\"\n",
        "        model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "        # Gerar texto\n",
        "        generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "        # Converter texto gerado para áudio\n",
        "        tts = gTTS(text=generated_text, lang='en')\n",
        "        audio_file = \"resposta.mp3\"\n",
        "        tts.save(audio_file)\n",
        "        playsound.playsound(audio_file)\n",
        "        os.remove(audio_file)\n",
        "\n",
        "        # Exibir a resposta\n",
        "        text_output.config(state=tk.NORMAL)\n",
        "        text_output.delete(\"1.0\", tk.END)\n",
        "        text_output.insert(tk.END, generated_text)\n",
        "        text_output.config(state=tk.DISABLED)\n",
        "\n",
        "        # Salvar as respostas\n",
        "        saveResponse(f\"\\nTópico Solicitado: {prompt}\", FILE_NAME)\n",
        "        saveResponse(f\"Texto gerado: {generated_text}\", FILE_NAME)\n",
        "    else:\n",
        "        text_output.config(state=tk.NORMAL)\n",
        "        text_output.delete(\"1.0\", tk.END)\n",
        "        text_output.insert(tk.END, \"Não foi recebida nenhuma entrada válida do microfone.\")\n",
        "        text_output.config(state=tk.DISABLED)\n",
        "\n",
        "def on_instructions_button_click():\n",
        "    instructions = (\n",
        "        \"Este chatbot permite que você faça perguntas por meio do microfone. \"\n",
        "        \"Pressione o botão 'Ativar Microfone', fale a sua pergunta, e o chatbot \"\n",
        "        \"irá processá-la e fornecer uma resposta. As mensagens de status, como \"\n",
        "        \"'Ajustando o ruído ambiente' e 'Reconhecendo...', são exibidas no Log \"\n",
        "        \"de Mensagens.\"\n",
        "    )\n",
        "    messagebox.showinfo(\"Instruções\", instructions)\n",
        "\n",
        "# Função para adicionar mensagens de log na interface\n",
        "def log_message(message):\n",
        "    log_text.configure(state=tk.NORMAL)\n",
        "    log_text.insert(tk.END, message + '\\n')\n",
        "    log_text.configure(state=tk.DISABLED)\n",
        "    log_text.yview(tk.END)\n",
        "    root.update_idletasks()  # Atualiza a interface para garantir que o log seja exibido\n",
        "\n",
        "# Configuração da interface gráfica\n",
        "root = tk.Tk()\n",
        "root.title(\"GPT-2 Question Answering\")\n",
        "root.geometry(\"800x600\")\n",
        "\n",
        "# Texto de introdução\n",
        "intro_label = tk.Label(root, text=\"GPT-2 Resposta Contexto\")\n",
        "intro_label.pack(padx=10, pady=10)\n",
        "\n",
        "# Botões e Log de Mensagens\n",
        "button_frame = tk.Frame(root)\n",
        "button_frame.pack(padx=10, pady=10)\n",
        "recognize_button = tk.Button(button_frame, text=\"Ativar Microfone\", command=activate_microphone)\n",
        "recognize_button.grid(row=0, column=0, padx=5)\n",
        "instructions_button = tk.Button(button_frame, text=\"Instruções\", command=on_instructions_button_click)\n",
        "instructions_button.grid(row=0, column=1, padx=5)\n",
        "\n",
        "log_label = tk.Label(root, text=\"Log de Mensagens:\")\n",
        "log_label.pack(anchor='w', padx=10)\n",
        "log_text = scrolledtext.ScrolledText(root, height=8, width=80, state=tk.DISABLED)\n",
        "log_text.pack(padx=10, pady=5)\n",
        "\n",
        "# Pergunta\n",
        "question_label = tk.Label(root, text=\"Pergunta:\")\n",
        "question_label.pack(anchor='w', padx=10)\n",
        "question_text = scrolledtext.ScrolledText(root, height=5, width=80, state=tk.DISABLED)\n",
        "question_text.pack(padx=10, pady=5)\n",
        "\n",
        "# Resposta Gerada\n",
        "text_output_label = tk.Label(root, text=\"Texto gerado:\")\n",
        "text_output_label.pack(anchor='w', padx=10)\n",
        "text_output = scrolledtext.ScrolledText(root, height=10, width=80, state=tk.DISABLED)\n",
        "text_output.pack(padx=10, pady=5)\n",
        "\n",
        "root.mainloop()\n"
      ],
      "metadata": {
        "id": "oQMXXCfOiL5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7amsZWHATgp"
      },
      "source": [
        "# Inferência GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TgAaba7L4wnd",
        "outputId": "0e720c04-fd08-4db9-9bbe-296ffbd277d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.7.4)\n",
            "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "euBxt5Qxe03m",
        "outputId": "82a75e9c-d6c8-4d0d-ed72-e69ae1f6e474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2024.7.4)\n",
            "Downloading gTTS-2.5.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVlEX53kS4YO",
        "outputId": "792b9542-9cbc-4d6b-dd32-d8696de08e81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conteúdo de áudio gravado em resposta.mp3\n",
            "\n",
            "Tópico solicitado: What are Festas Juninas and where are they most commonly celebrated in Brazil?\n",
            "\n",
            "Texto gerado: Festas are typical celebrations in the month of June in Brazil, with traditional foods such as corn, pamonha, and quentão.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "# Salvar as respostas das perguntas\n",
        "def saveResponse(text, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "FILE_NAME = \"respostas_voz.txt\"\n",
        "\n",
        "def listen_microphone():\n",
        "    # Texto para simular a entrada de áudio\n",
        "    return \"What are Festas Juninas and where are they most commonly celebrated in Brazil?\"\n",
        "\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=500, temperature=0.5, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    if generated_text.startswith(prompt):\n",
        "        generated_text = generated_text[len(prompt):].strip()\n",
        "    return generated_text\n",
        "\n",
        "def main():\n",
        "    # Escutar o microfone (modificado para texto fixo)\n",
        "    prompt = listen_microphone()\n",
        "\n",
        "    if prompt:\n",
        "        # Carregar modelo e tokenizer ajustado\n",
        "        model_name = \"fine_tuned_gpt2\"\n",
        "        model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "        # Gerar texto\n",
        "        generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "        # Converter texto gerado para áudio\n",
        "        tts = gTTS(text=generated_text, lang='en')\n",
        "        tts.save(\"resposta.mp3\")\n",
        "        print(\"Conteúdo de áudio gravado em resposta.mp3\")\n",
        "\n",
        "        # Imprimir os resultados\n",
        "        print(\"\\nTópico solicitado:\", prompt)\n",
        "        print(\"\\nTexto gerado:\", generated_text)\n",
        "\n",
        "        saveResponse(f\"\\nTópico Solicitado: {prompt}\", FILE_NAME)\n",
        "        saveResponse(f\"Texto gerado: {generated_text}\", FILE_NAME)\n",
        "\n",
        "    else:\n",
        "        print(\"Não foi recebida nenhuma entrada válida do microfone.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYnlRcIJAYXy",
        "outputId": "2fbb9f2b-ef4a-46a4-d4f4-b4adb9592417"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tópico Solicitado: Fale sobre IA\n",
            "Texto gerado: Fale sobre IA é uma decisões da UNEB. A é um grupo de robótica da computação geral de tarefas da execuço. É umar código projetado para ufide de forma autônoma. Isso inteligência inteligencia e legibilidade e inteligentes intelibes e formada. Ele é que significa que se formas anteriores e experiências de inteliga humanoide. Além inteligo intelilade inteliò não intelisa e humana. E sistemas intelisigibiles e são proporciona, intel\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "# Salvar as respostas das perguntas\n",
        "def saveResponse(text, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(text + '\\n')\n",
        "\n",
        "FILE_NAME = \"respostas_voz.txt\"\n",
        "\n",
        "def listen_microphone():\n",
        "    # Texto para simular a entrada de áudio\n",
        "    return \"Fale sobre Robôs de Serviço\"\n",
        "\n",
        "def load_model_and_tokenizer(model_name):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=500, temperature=0.2, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    if generated_text.startswith(prompt):\n",
        "        generated_text = generated_text[len(prompt):].strip()\n",
        "    return generated_text\n",
        "\n",
        "def main():\n",
        "    # Escutar o microfone (modificado para texto fixo)\n",
        "    prompt = listen_microphone()\n",
        "\n",
        "    if prompt:\n",
        "        # Carregar modelo e tokenizer ajustado\n",
        "        model_name = \"fine_tuned_gpt2\"\n",
        "        model, tokenizer = load_model_and_tokenizer(model_name)\n",
        "\n",
        "        # Gerar texto\n",
        "        generated_text = generate_text(model, tokenizer, prompt)\n",
        "\n",
        "        # Converter texto gerado para áudio\n",
        "        tts = gTTS(text=generated_text, lang='pt')\n",
        "        tts.save(\"resposta.mp3\")\n",
        "        print(\"Conteúdo de áudio gravado em resposta.mp3\")\n",
        "\n",
        "        # Imprimir os resultados\n",
        "        print(\"\\nTópico solicitado:\", prompt)\n",
        "        print(\"\\nTexto gerado:\", generated_text)\n",
        "\n",
        "        saveResponse(f\"\\nTópico Solicitado: {prompt}\", FILE_NAME)\n",
        "        saveResponse(f\"Texto gerado: {generated_text}\", FILE_NAME)\n",
        "\n",
        "    else:\n",
        "        print(\"Não foi recebida nenhuma entrada válida do microfone.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1PzTlahN1qN"
      },
      "source": [
        "o algoritmo de inferência possibila a realização de perguntas com base do contexto determinado\n",
        "2 argumentos são passados\n",
        "contexto e perguntas relacionadas ao contexto\n",
        "os dados são pré-processados, tokenizados e convertidos em palavras, usando o tokenizador\n",
        "modelo produz 2 saídas\n",
        "pontuações de início (start_scores)\n",
        "probabilidade de cada token no contexto ser o início da resposta\n",
        "pontuações de fim (end_scores)\n",
        "probabilidade de cada token no contexto ser o fim da resposta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3dImZOI3iF0J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_oPd7gK5WXs"
      },
      "source": [
        "# Voz com palavra-chave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rgICp7Hcjua5",
        "outputId": "da09dc1a-314a-4e32-88de-bf95f31d6957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.5)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.33.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.6.20)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.33.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import playsound\n",
        "\n",
        "# Save question and answer in file\n",
        "def save_response(question, answer, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(f\"Question: {question}\\n\")\n",
        "        file.write(f\"Answer: {answer}\\n\")\n",
        "        file.write(\"\\n\")\n",
        "\n",
        "FILE_NAME = \"answer.txt\"\n",
        "\n",
        "# Load fine-tuned GPT-2 model and tokenizer\n",
        "def load_model_and_tokenizer(model_dir):\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
        "    return model, tokenizer\n",
        "\n",
        "model_dir = 'fine_tuned_gpt2'\n",
        "model, tokenizer = load_model_and_tokenizer(model_dir)\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=150, temperature=0.7, top_p=0.9):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            no_repeat_ngram_size=3,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=True  # Enable sampling to use temperature and top_p\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    # Remove the initial prompt text if it's repeated in the generated text\n",
        "    if generated_text.startswith(prompt):\n",
        "        generated_text = generated_text[len(prompt):].strip()\n",
        "\n",
        "    # Clean up text to remove extra characters\n",
        "    generated_text = generated_text.replace('?', '').strip()\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "def wait_for_keyword(keywords, recognizer):\n",
        "    while True:\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Adjusting the ambient noise. Please wait...\")\n",
        "            recognizer.adjust_for_ambient_noise(source)\n",
        "            print(\"Waiting for Keyword...\")\n",
        "            audio_file = \"microphoneOn.mp3\"\n",
        "            playsound.playsound(audio_file)\n",
        "            audio = recognizer.listen(source)\n",
        "\n",
        "        try:\n",
        "            print(\"Recognizing...\")\n",
        "            text = recognizer.recognize_google(audio, language='en')\n",
        "            print(\"You said: \", text)\n",
        "\n",
        "            if any(keyword.lower() in text.lower() for keyword in keywords):\n",
        "                print(\"Keyword detected!\")\n",
        "                return True\n",
        "            else:\n",
        "                print(\"Keyword not detected. Please try again.\")\n",
        "\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"I don't understand what you said. Please, repeat.\")\n",
        "        except sr.RequestError as e:\n",
        "            print(f\"Error in the request to the Google API: {e}\")\n",
        "\n",
        "def recognize_speech_from_audio():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Adjusting the ambient noise. Please wait...\")\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "\n",
        "        print(\"Please ask your question...\")\n",
        "        audio_file = \"microphoneOn.mp3\"\n",
        "        playsound.playsound(audio_file)\n",
        "\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        print(\"Recognizing...\")\n",
        "        return recognizer.recognize_google(audio, language=\"en\")\n",
        "    except sr.UnknownValueError:\n",
        "        return \"I don't understand you\"\n",
        "    except sr.RequestError:\n",
        "        return \"Error connecting to speech recognition service.\"\n",
        "\n",
        "def play_response(answer_text):\n",
        "    if answer_text.strip():\n",
        "        tts = gTTS(text=answer_text, lang=\"en\")\n",
        "        audio_file = \"answer.mp3\"\n",
        "        tts.save(audio_file)\n",
        "        playsound.playsound(audio_file)\n",
        "        os.remove(audio_file)\n",
        "    else:\n",
        "        print(\"Empty response generated. Unable to play audio\")\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        question = recognize_speech_from_audio()\n",
        "        print(f\"Recognized Question: {question}\")\n",
        "\n",
        "        if \"I don't understand you\" in question:\n",
        "            print(\"Could not understand the question. Please try again.\")\n",
        "            continue\n",
        "\n",
        "        # Use a more specific prompt to generate a relevant answer\n",
        "        prompt = f\"Answer the following question: {question}\"\n",
        "        answer = generate_text(model, tokenizer, prompt)\n",
        "        print(f\"Generated answer: {answer}\")\n",
        "\n",
        "        save_response(question, answer, FILE_NAME)\n",
        "\n",
        "        if answer.strip():\n",
        "            play_response(answer)\n",
        "        else:\n",
        "            print(\"The model did not generate a valid response. Check the prompt and model.\")\n",
        "\n",
        "def listen_microphone():\n",
        "    keywords = [\"Bill\", \"Hi Bill\", \"Ok Bill\", \"ok\"]\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    while True:\n",
        "        if wait_for_keyword(keywords, recognizer):\n",
        "            main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    listen_microphone()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgOG0PP5tEu0"
      },
      "source": [
        "# QA BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HVtjaahtHiZ",
        "outputId": "8fc204cd-82b5-4ced-94d3-e2391607c078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No known TensorBoard instances running.\n"
          ]
        }
      ],
      "source": [
        "from tensorboard import notebook\n",
        "notebook.list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWvuex-vtUiV",
        "outputId": "18065fc5-e921-44a1-d10f-bdac5dfcfec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-14 18:25:58.302300: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-14 18:25:58.337183: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-14 18:25:58.345445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-14 18:25:58.369047: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-08-14 18:26:01.729739: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "======================================================================\n",
            "ERROR: The `tensorboard dev` command is no longer available.\n",
            "\n",
            "TensorBoard.dev has been shut down. For further information,\n",
            "see the FAQ at <https://tensorboard.dev/>.\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "!tensorboard dev upload \\\n",
        "  --logdir \"/content/runs\" \\\n",
        "  --name \"(BertQA2)\" \\\n",
        "  --description \"BertQA for Robotics\" \\\n",
        "  --one_shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "collapsed": true,
        "id": "H0_oyXfRtXnV",
        "outputId": "f1e77e58-f809-4b9d-84a0-2d07daf4355b"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2L7ddtG9KdDH",
        "outputId": "bdb5f6a1-08b2-457a-9b78-8df81c161d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2.0, Loss: 0.8914\n",
            "Validation Accuracy: 0.0000\n",
            "Epoch 2/2.0, Loss: 0.7224\n",
            "Validation Accuracy: 0.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('fine_tuned_distilbert/tokenizer_config.json',\n",
              " 'fine_tuned_distilbert/special_tokens_map.json',\n",
              " 'fine_tuned_distilbert/vocab.txt',\n",
              " 'fine_tuned_distilbert/added_tokens.json')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "\n",
        "# Definir hiperparâmetros de treinamento\n",
        "learning_rate = 3e-05\n",
        "train_batch_size = 12\n",
        "eval_batch_size = 8\n",
        "seed = 42\n",
        "optimizer_betas = (0.9, 0.999)\n",
        "optimizer_epsilon = 1e-08\n",
        "lr_scheduler_type = \"linear\"\n",
        "num_epochs = 2.0\n",
        "\n",
        "# Crie o conjunto de dados de perguntas e respostas sobre Valorant.\n",
        "# Triplique o conjunto de dados sobre Valorant.\n",
        "df = pd.DataFrame({\n",
        "    \"Pergunta\": [\n",
        "        \"Como funcionam os robôs autônomos na indústria?\",\n",
        "        \"Quais são os sensores mais comuns em robótica?\",\n",
        "        \"Qual é a história da robótica industrial?\",\n",
        "        \"Quais são as estratégias de navegação em robôs móveis?\",\n",
        "        \"Qual é o melhor design para um robô de serviço?\",\n",
        "        \"Como usar efetivamente a visão computacional em robótica?\",\n",
        "        \"Quais são as diferenças entre robôs industriais e robôs colaborativos?\",\n",
        "        \"Como funciona o aprendizado de máquina em robótica?\",\n",
        "        \"Quais são as habilidades de um robô autônomo?\",\n",
        "        \"Como economizar energia em robôs móveis?\",\n",
        "        \"Quais são os melhores algoritmos de localização para robôs?\",\n",
        "        \"Como prever o comportamento de robôs em ambientes dinâmicos?\",\n",
        "        \"Quais são as estratégias para otimizar a produção em robótica industrial?\",\n",
        "        \"Qual é a função de um manipulador robótico?\",\n",
        "        \"Quais são os melhores atuadores para movimento em robótica?\",\n",
        "        \"Como usar os sensores táteis em robótica de forma eficaz?\",\n",
        "        \"Quais são as dicas para programar robôs colaborativos?\",\n",
        "        \"Como identificar a melhor solução robótica para uma tarefa específica?\",\n",
        "        \"Quais são as características de uma interface homem-máquina eficaz em robótica?\",\n",
        "        \"Como funciona o sistema de simulação em robótica?\"\n",
        "    ],\n",
        "    \"Resposta\": [\n",
        "        \"Os robôs autônomos na indústria operam de forma independente, tomando decisões com base em sensores e algoritmos.\",\n",
        "        \"Sensores comuns em robótica incluem câmeras, sensores de proximidade e sensores inerciais.\",\n",
        "        \"A história da robótica industrial remonta a meados do século 20, com o surgimento dos primeiros robôs industriais.\",\n",
        "        \"Estratégias de navegação em robôs móveis podem envolver SLAM (Simultaneous Localization and Mapping) e planejamento de trajetória.\",\n",
        "        \"O design de um robô de serviço depende da aplicação, com considerações de mobilidade, alcance e carga útil.\",\n",
        "        \"A visão computacional é essencial para a percepção visual em robótica, permitindo que os robôs entendam o ambiente.\",\n",
        "        \"Robôs industriais são usados em ambientes de produção, enquanto robôs colaborativos interagem com humanos de forma segura.\",\n",
        "        \"O aprendizado de máquina em robótica permite que os robôs aprendam e melhorem suas habilidades com o tempo.\",\n",
        "        \"Habilidades de um robô autônomo incluem navegação, interação com objetos e tomada de decisões baseada em dados.\",\n",
        "        \"Economizar energia em robôs móveis envolve otimização de baterias e uso eficiente de recursos.\",\n",
        "        \"Algoritmos de localização, como o filtro de partículas, são usados para determinar a posição de um robô em um ambiente.\",\n",
        "        \"Prever o comportamento de robôs em ambientes dinâmicos requer modelagem e planejamento avançado.\",\n",
        "        \"Estratégias de otimização de produção na robótica industrial visam aumentar a eficiência e a qualidade da produção.\",\n",
        "        \"Um manipulador robótico é responsável por realizar tarefas de manipulação, como pegar e mover objetos.\",\n",
        "        \"Atuadores, como motores elétricos e pneumáticos, são usados para criar movimento em robôs.\",\n",
        "        \"Sensores táteis permitem que os robôs detectem e respondam ao toque e à pressão em suas superfícies.\",\n",
        "        \"Programar robôs colaborativos requer considerações de segurança e interface de usuário intuitiva.\",\n",
        "        \"Escolher a melhor solução robótica envolve avaliar requisitos, orçamento e aplicação específica.\",\n",
        "        \"Uma interface homem-máquina eficaz facilita a interação entre humanos e robôs de forma intuitiva.\",\n",
        "        \"Sistemas de simulação em robótica permitem testar algoritmos e comportamento de robôs em ambientes virtuais.\"\n",
        "    ],\n",
        "    \"Texto\": [\n",
        "        \"Os robôs autônomos são essenciais para a automação industrial e a eficiência de processos.\",\n",
        "        \"Sensores desempenham um papel fundamental na percepção do ambiente por parte dos robôs.\",\n",
        "        \"A robótica industrial tem evoluído ao longo das décadas, transformando a produção em muitas indústrias.\",\n",
        "        \"A navegação precisa é crucial para robôs móveis que operam em ambientes dinâmicos.\",\n",
        "        \"O design de robôs de serviço varia de acordo com as necessidades da tarefa a ser executada.\",\n",
        "        \"A visão computacional permite que os robôs interpretem informações visuais de seu ambiente.\",\n",
        "        \"Robôs colaborativos estão se tornando cada vez mais comuns em ambientes de trabalho compartilhados.\",\n",
        "        \"O aprendizado de máquina capacita os robôs a melhorarem suas habilidades com base em experiências passadas.\",\n",
        "        \"Robôs autônomos podem executar uma ampla gama de tarefas, desde navegação até reconhecimento de objetos.\",\n",
        "        \"A eficiência energética é fundamental para prolongar a autonomia dos robôs móveis.\",\n",
        "        \"Algoritmos de localização desempenham um papel fundamental na navegação precisa de robôs.\",\n",
        "        \"Prever o comportamento de robôs é importante para a segurança e o desempenho em ambientes dinâmicos.\",\n",
        "        \"A otimização da produção é um objetivo-chave na automação industrial com robôs.\",\n",
        "        \"Manipuladores robóticos desempenham um papel importante na montagem e na manipulação de objetos.\",\n",
        "        \"Atuadores são os motores que conferem movimento e ação aos robôs.\",\n",
        "        \"Sensores táteis permitem que os robôs percebam e reajam a interações físicas com o ambiente.\",\n",
        "        \"Programar robôs colaborativos envolve garantir que eles possam interagir com segurança com humanos.\",\n",
        "        \"Escolher a solução robótica certa é crucial para atender às necessidades específicas da aplicação.\",\n",
        "        \"Uma interface homem-máquina eficaz facilita a operação e a colaboração entre humanos e robôs.\",\n",
        "        \"A simulação é uma ferramenta valiosa para o desenvolvimento e teste de algoritmos robóticos.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "df_extra2 = pd.DataFrame({\n",
        "    \"Pergunta\": [\n",
        "        \"Quais são as aplicações de robótica na indústria aeroespacial?\",\n",
        "        \"Como os drones estão sendo utilizados em operações de robótica?\",\n",
        "        \"Quais são os desafios da robótica em ambientes extremos, como o espaço?\",\n",
        "        \"Como a robótica está transformando a logística e a cadeia de suprimentos?\",\n",
        "        \"Quais são as implicações da robótica na criação de empregos?\",\n",
        "        \"Como os robôs são programados para reconhecimento de voz e linguagem natural?\",\n",
        "        \"Quais são as inovações em robótica biomimética?\",\n",
        "        \"Como a robótica está sendo aplicada na educação?\",\n",
        "        \"Quais são os riscos de segurança associados à automação robótica?\",\n",
        "        \"Como os exoesqueletos robóticos estão sendo utilizados em aplicações industriais?\",\n",
        "        \"Quais são os avanços em robótica cirúrgica?\",\n",
        "        \"Como os robôs são utilizados na manutenção preditiva de equipamentos industriais?\",\n",
        "        \"Quais são as tendências em robótica de entretenimento e jogos?\",\n",
        "        \"Como a ética na inteligência artificial se relaciona com a robótica autônoma?\",\n",
        "        \"Quais são os benefícios da robótica na exploração subterrânea?\",\n",
        "        \"Como os robôs colaborativos estão sendo integrados em linhas de produção?\",\n",
        "        \"Quais são os desafios da robótica social na interação com seres humanos?\",\n",
        "        \"Como a robótica está impactando a fabricação de dispositivos médicos?\",\n",
        "        \"Quais são as considerações éticas em robótica militar?\",\n",
        "        \"Como a robótica está sendo utilizada na conservação ambiental?\"\n",
        "    ],\n",
        "    \"Resposta\": [\n",
        "        \"A robótica na indústria aeroespacial é amplamente utilizada em montagem, inspeção e manutenção de aeronaves.\",\n",
        "        \"Drones desempenham papéis cruciais em diversas operações, como monitoramento ambiental, entregas e inspeções.\",\n",
        "        \"Ambientes extremos, como o espaço, apresentam desafios únicos, incluindo radiação e condições de microgravidade.\",\n",
        "        \"A robótica transforma a logística ao automatizar processos de armazenamento, picking e embalagem em centros de distribuição.\",\n",
        "        \"A automação robótica pode criar empregos especializados, mas também levanta preocupações sobre a substituição de empregos tradicionais.\",\n",
        "        \"Programação para reconhecimento de voz e linguagem natural envolve algoritmos avançados de processamento de linguagem natural.\",\n",
        "        \"A robótica biomimética se inspira na natureza para desenvolver robôs com características semelhantes a animais e organismos.\",\n",
        "        \"A robótica na educação inclui kits de robótica para ensinar programação e conceitos de engenharia de forma prática.\",\n",
        "        \"Riscos de segurança em automação robótica incluem falhas técnicas, hacking e questões éticas relacionadas à segurança humana.\",\n",
        "        \"Exoesqueletos robóticos são usados em indústrias para melhorar a ergonomia e reduzir o risco de lesões musculoesqueléticas.\",\n",
        "        \"Avanços em robótica cirúrgica incluem sistemas minimamente invasivos e aprimoramentos em técnicas de precisão.\",\n",
        "        \"Robôs na manutenção preditiva monitoram o estado de equipamentos, identificando e corrigindo problemas antes que ocorram falhas.\",\n",
        "        \"Tendências em robótica de entretenimento incluem robôs interativos e inovações em realidade virtual e aumentada.\",\n",
        "        \"A ética na inteligência artificial e robótica autônoma envolve questões de transparência, responsabilidade e bias algorítmico.\",\n",
        "        \"Robótica na exploração subterrânea é crucial para tarefas como mapeamento de minas e inspeção de infraestrutura.\",\n",
        "        \"Robôs colaborativos são integrados em linhas de produção para trabalhar lado a lado com humanos, aumentando eficiência.\",\n",
        "        \"Desafios da robótica social incluem compreensão emocional, interação natural e considerações éticas na colaboração.\",\n",
        "        \"A robótica impacta a fabricação de dispositivos médicos, melhorando precisão em procedimentos e automação em produção.\",\n",
        "        \"Considerações éticas em robótica militar envolvem autonomia, proporcionalidade e minimização de danos colaterais.\",\n",
        "        \"Robótica na conservação ambiental é utilizada em monitoramento de vida selvagem, limpeza de resíduos e restauração de ecossistemas.\"\n",
        "    ],\n",
        "    \"Texto\": [\n",
        "        \"Robótica aeroespacial contribui para a eficiência e segurança na manutenção de aeronaves e veículos espaciais.\",\n",
        "        \"Drones são ferramentas versáteis em operações, desde inspeções de infraestrutura até entregas de emergência.\",\n",
        "        \"Explorar ambientes extremos, como o espaço, exige robôs adaptados para suportar condições únicas e desafios técnicos.\",\n",
        "        \"Automatizar a logística com robótica melhora a eficiência, reduzindo erros e acelerando o atendimento a pedidos.\",\n",
        "        \"O impacto da automação na criação de empregos é um tópico debatido, com vantagens e desafios a serem considerados.\",\n",
        "        \"Reconhecimento de voz em robótica permite interações mais naturais, enquanto algoritmos de linguagem natural melhoram a compreensão.\",\n",
        "        \"Biomimética na robótica busca inspiração na natureza para criar soluções eficientes e adaptáveis.\",\n",
        "        \"Robótica na educação estimula o aprendizado prático de conceitos STEM (Ciência, Tecnologia, Engenharia e Matemática).\",\n",
        "        \"Riscos de segurança em robótica destacam a importância de medidas preventivas e éticas na implementação de sistemas autônomos.\",\n",
        "        \"Exoesqueletos industriais suportam trabalhadores, reduzindo fadiga e melhorando a segurança em ambientes laborais.\",\n",
        "        \"Robótica cirúrgica avançada permite procedimentos menos invasivos e precisos, melhorando a recuperação do paciente.\",\n",
        "        \"Manutenção preditiva com robôs previne falhas, reduzindo o tempo de inatividade e os custos associados à manutenção corretiva.\",\n",
        "        \"Entretenimento robótico inova com interações imersivas, promovendo experiências únicas para usuários.\",\n",
        "        \"Ética em inteligência artificial é fundamental para garantir decisões justas e transparentes em sistemas autônomos.\",\n",
        "        \"Exploração subterrânea com robôs é essencial para acessar áreas de difícil alcance e realizar tarefas específicas.\",\n",
        "        \"Robôs colaborativos em linhas de produção combinam eficiência e segurança, permitindo a coexistência com trabalhadores.\",\n",
        "        \"Robótica social busca superar desafios na interação robô-humano, priorizando a compreensão emocional e o respeito ético.\",\n",
        "        \"Impacto da robótica na fabricação de dispositivos médicos inclui melhorias em precisão e consistência na produção.\",\n",
        "        \"Ética em robótica militar envolve considerações sobre o uso responsável de tecnologia em contextos de conflito.\",\n",
        "        \"Robótica na conservação ambiental contribui para a preservação da biodiversidade e a restauração de ecossistemas.\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Concatenando os DataFrames\n",
        "df = pd.concat([df, df_extra2] * 3, ignore_index=True)\n",
        "\n",
        "# Dividir o conjunto de dados em treinamento e validação\n",
        "train_df, validation_df = train_test_split(df, test_size=0.5, random_state=seed)\n",
        "\n",
        "# Importar o modelo DistilBERT e o tokenizer\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Adicionar dropout e camada linear adicional ao modelo\n",
        "dropout_prob = 0.3\n",
        "model.distilbert.transformer.layer[-1].output = nn.Sequential(\n",
        "    nn.Dropout(dropout_prob),\n",
        "    nn.Linear(model.config.dim, model.config.dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(dropout_prob),\n",
        "    nn.Linear(model.config.dim, model.config.num_labels),\n",
        "    nn.Linear(model.config.num_labels, model.config.num_labels)  # Camada adicional\n",
        ")\n",
        "\n",
        "# Adicionar função de ativação Softmax no final do modelo\n",
        "class CustomDistilBERTForQuestionAnswering(DistilBertForQuestionAnswering):\n",
        "    def forward(self, **inputs):\n",
        "        outputs = super().forward(**inputs)\n",
        "        start_logits = outputs.start_logits\n",
        "        end_logits = outputs.end_logits\n",
        "\n",
        "        # Aplicar função de ativação softmax para obter probabilidades\n",
        "        start_probs = F.softmax(start_logits, dim=1)\n",
        "        end_probs = F.softmax(end_logits, dim=1)\n",
        "\n",
        "        # Retornar probabilidades em vez de logits\n",
        "        return start_probs, end_probs\n",
        "\n",
        "# Criar uma instância do modelo personalizado\n",
        "model = CustomDistilBERTForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "\n",
        "# Definir otimizador e taxa de aprendizado\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=optimizer_betas, eps=optimizer_epsilon)\n",
        "\n",
        "# Definir o programador de taxa de aprendizado\n",
        "num_training_steps = len(train_df) * num_epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: 1 - step / num_training_steps)\n",
        "\n",
        "# Listas para armazenar métricas de treinamento e validação\n",
        "train_losses = []\n",
        "validation_losses = []\n",
        "accuracies = []\n",
        "\n",
        "# Loop de treinamento\n",
        "for epoch in range(int(num_epochs)):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i in range(0, len(train_df), train_batch_size):\n",
        "        batch_df = train_df.iloc[i:i+train_batch_size]\n",
        "\n",
        "        inputs = tokenizer(batch_df[\"Pergunta\"].tolist(), batch_df[\"Texto\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = model(**inputs)\n",
        "        start_probs = outputs[0]\n",
        "        end_probs = outputs[1]\n",
        "\n",
        "        # Calcular a perda com base nas probabilidades de início e fim\n",
        "        loss = -torch.log(start_probs) - torch.log(end_probs)\n",
        "        loss = loss.mean()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Retropropagação e otimização\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    # Calcular a perda média\n",
        "    average_loss = total_loss / len(train_df)\n",
        "    train_losses.append(average_loss)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "    # Validar o modelo no conjunto de validação\n",
        "    model.eval()\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    for i in range(0, len(validation_df), eval_batch_size):\n",
        "        batch_df = validation_df.iloc[i:i+eval_batch_size]\n",
        "\n",
        "        inputs = tokenizer(batch_df[\"Pergunta\"].tolist(), batch_df[\"Texto\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = model(**inputs)\n",
        "        start_probs = outputs[0]\n",
        "        end_probs = outputs[1]\n",
        "\n",
        "        # Encontrar as posições de início e fim preditas\n",
        "        start_pred = torch.argmax(start_probs, dim=1)\n",
        "        end_pred = torch.argmax(end_probs, dim=1)\n",
        "\n",
        "        # Obter as respostas preditas\n",
        "        predicted_answers = [tokenizer.decode(input_ids[start:end+1]) for input_ids, start, end in zip(inputs[\"input_ids\"], start_pred, end_pred)]\n",
        "\n",
        "        # Calcular a acurácia\n",
        "        true_answers = batch_df[\"Resposta\"].tolist()\n",
        "        accuracy = sum(1.0 if pred == true else 0.0 for pred, true in zip(predicted_answers, true_answers))\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "    # Calcular a acurácia média\n",
        "    average_accuracy = total_accuracy / len(validation_df)\n",
        "    accuracies.append(average_accuracy)\n",
        "    print(f\"Validation Accuracy: {average_accuracy:.4f}\")\n",
        "\n",
        "# Salvar o modelo treinado\n",
        "model.save_pretrained(\"fine_tuned_distilbert\")\n",
        "# Salvar o tokenizer separadamente\n",
        "tokenizer.save_pretrained(\"fine_tuned_distilbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5GDaSgChtg8Q",
        "outputId": "5f479de3-0016-4868-a75e-54fda498cb01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.4.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "59047e0303e34e729913ff9556753c29",
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install datasets transformers torch scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFzULANvD5Ps"
      },
      "source": [
        "# Inferência QABERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2H_bNG2Egzl",
        "outputId": "2aad9d65-05c0-40c9-ae1b-59f457107ab5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tkinter (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tkinter\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting speechrecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from speechrecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from speechrecognition) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (2024.7.4)\n",
            "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: speechrecognition\n",
            "Successfully installed speechrecognition-3.10.4\n",
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2024.7.4)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.3\n",
            "Collecting playsound\n",
            "  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: playsound\n",
            "  Building wheel for playsound (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7020 sha256=8ceb00eeec5e094bcbd412aabdd17c2f9b4ad3bdbb233f9da56baa2ea36c52da\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/89/ed/2d643f4226fc8c7c9156fc28abd8051e2d2c0de37ae51ac45c\n",
            "Successfully built playsound\n",
            "Installing collected packages: playsound\n",
            "Successfully installed playsound-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tkinter\n",
        "!pip install speechrecognition\n",
        "!pip install gTTS\n",
        "!pip install playsound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "mG47D4xqD5EN",
        "outputId": "f74ae382-20c1-4e7b-870d-9c1bfdb8a65d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "fine_tuned_distilbert is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/fine_tuned_distilbert/resolve/main/config.json",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;31m# Repo not found or gated => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m                 \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hf_file_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEntryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1646\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    351\u001b[0m             )\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-66bdf859-2c92935a3bdd5f6962d42d18;e73e35c2-834b-4096-aea8-0e37613c9edb)\n\nRepository Not Found for url: https://huggingface.co/fine_tuned_distilbert/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-55a0cdc30b11>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fine_tuned_distilbert\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fine_tuned_distilbert\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3225\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   3226\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m         ) from e\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: fine_tuned_distilbert is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"fine_tuned_distilbert\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"fine_tuned_distilbert\")\n",
        "\n",
        "\n",
        "def ask_question(context, question, temperature=1.0, min_confidence=0.2, max_retries=5):\n",
        "    for _ in range(max_retries):\n",
        "        inputs = tokenizer(question, context, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = model(**inputs)\n",
        "        start_scores = outputs.start_logits / temperature\n",
        "        end_scores = outputs.end_logits / temperature\n",
        "\n",
        "        # função softmax aos scores\n",
        "        start_probs = torch.softmax(start_scores, dim=1)\n",
        "        end_probs = torch.softmax(end_scores, dim=1)\n",
        "\n",
        "        # Amostragem ponderada para obter índices de início e fim\n",
        "        start_index = torch.multinomial(start_probs, 1).item()\n",
        "        end_index = torch.multinomial(end_probs, 1).item() + 1\n",
        "\n",
        "        answer = tokenizer.decode(inputs.input_ids[0][start_index:end_index])\n",
        "\n",
        "        # Verifique se a resposta não está vazia\n",
        "        if answer.strip():\n",
        "            return answer\n",
        "\n",
        "    # Se após várias tentativas não for possível obter uma resposta não vazia, retorne uma mensagem padrão\n",
        "    return \"Não foi possível encontrar uma resposta relevante.\"\n",
        "\n",
        "\n",
        "context = (\"Bahia is a Brazilian state located in the Northeast Region. The population of Bahia is 14.9 million inhabitants, the fourth largest in Brazil. The municipality of Salvador is the state capital and also served as the first Brazilian capital between 1549 and 1763. The state's geography is characterized by plains and depressions, as well as two dominant climate types, tropical and semi-arid. Bahia's economy is the main one in the Northeast and focuses on the tertiary sector and the manufacturing industry. Bahia climate Two climate types are predominant in Bahia. The first is the semi-arid, which occurs in the central region (with the exception of the highest areas) and part of the west of the state. This climate is characterized by high temperatures and low relative humidity, with irregular precipitation and a dry period generally in the winter months. Rainfall is concentrated in the summer, with an annual average of around 800 mm. The tropical climate occurs mainly in the east of Bahia, also marked by high annual temperatures and a higher level of humidity, especially on the coast. Average annual precipitation varies between 1,200 and 1,600 mm. In some coastal cities, this value can exceed 2,000 mm. Relief of Bahia Plateaus and depressions are the predominant forms of relief in Bahia, which gives most of the state elevations above 300 meters. The west of Bahia and part of the north are part of the Depression Sertaneja and São Francisco, to the east of which the Plateaus and Serras do Leste-Sudeste extend. The highest terrain in the state is located in this domain, with emphasis on Serra do Espinhaço and Chapada Diamantina. This is where the highest point of Bahia and the Northeast is located: Pico do Barbado, 2,033 meters above sea level. In the eastern range, finally, we have the Coastal Plains and Tablelands\")\n",
        "\n",
        "question = \"What's the population of Bahia?\"\n",
        "\n",
        "\n",
        "\n",
        "answer = ask_question(context, question, temperature=0.6)\n",
        "print(\"Pergunta:\", question)\n",
        "print(\"\\nResposta (Temperatura 0.6):\", answer)\n",
        "\n",
        "answer = ask_question(context, question, temperature=0.8)\n",
        "print(\"\\nResposta (Temperatura 0.8):\", answer)\n",
        "\n",
        "answer = ask_question(context, question, temperature=1.0)\n",
        "print(\"\\nResposta (Temperatura 1.0):\", answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clehGbD3ltW4"
      },
      "source": [
        "# Interface QABERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4rhUqWAlsYN"
      },
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "from tkinter import scrolledtext, messagebox\n",
        "import torch\n",
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import playsound\n",
        "\n",
        "# Função para salvar a pergunta e a resposta no arquivo\n",
        "def saveResponse(question, answer, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(f\"Pergunta: {question}\\n\")\n",
        "        file.write(f\"Resposta: {answer}\\n\")\n",
        "        file.write(\"\\n\")  # Adiciona uma linha em branco para separar as entradas\n",
        "\n",
        "FILE_NAME = \"respostas_voz.txt\"\n",
        "\n",
        "# Carregar o modelo e o tokenizador do DistilBERT\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def generate_answer(context, question):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "\n",
        "    # Obter as pontuações das posições da resposta\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    # Identificar as posições de início e fim da resposta\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    # Converter a resposta em texto\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    # Corrigir possíveis problemas com espaços e tokens\n",
        "    answer = answer.strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "def recognize_speech_from_audio():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        log_message(\"Ajustando o ruído ambiente. Por favor, aguarde...\")\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "        log_message(\"Por favor, faça sua pergunta...\")\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        log_message(\"Reconhecendo...\")\n",
        "        root.update_idletasks()  # Atualiza a interface enquanto o reconhecimento ocorre\n",
        "        return recognizer.recognize_google(audio, language=\"en\")\n",
        "    except sr.UnknownValueError:\n",
        "        return \"Não consegui entender o áudio.\"\n",
        "    except sr.RequestError:\n",
        "        return \"Erro ao se conectar ao serviço de reconhecimento de fala.\"\n",
        "\n",
        "def play_response_audio(answer_text):\n",
        "    if answer_text.strip():  # Verifica se a resposta não está vazia\n",
        "        tts = gTTS(text=answer_text, lang='en')\n",
        "        audio_file = \"resposta.mp3\"\n",
        "        tts.save(audio_file)\n",
        "        playsound.playsound(audio_file)\n",
        "        os.remove(audio_file)\n",
        "    else:\n",
        "        log_message(\"Resposta vazia gerada. Não é possível reproduzir o áudio.\")\n",
        "\n",
        "def on_recognize_button_click():\n",
        "    question = recognize_speech_from_audio()\n",
        "    question_entry.delete(0, tk.END)\n",
        "    question_entry.insert(0, question)\n",
        "    answer = generate_answer(context_text.get(\"1.0\", tk.END).strip(), question)\n",
        "\n",
        "    # Salvar a pergunta e a resposta no histórico\n",
        "    saveResponse(question, answer, FILE_NAME)\n",
        "    history_text.configure(state=tk.NORMAL)\n",
        "    history_text.insert(tk.END, f\"Pergunta: {question}\\nResposta: {answer}\\n\\n\")\n",
        "    history_text.configure(state=tk.DISABLED)\n",
        "\n",
        "    # Reproduzir a resposta em áudio\n",
        "    play_response_audio(answer)\n",
        "\n",
        "def on_instructions_button_click():\n",
        "    instructions = (\n",
        "        \"Este chatbot permite que você faça perguntas por meio do microfone. \"\n",
        "        \"Pressione o botão 'Ativar Microfone', fale a sua pergunta, e o chatbot \"\n",
        "        \"irá processá-la e fornecer uma resposta. As mensagens de status, como \"\n",
        "        \"'Ajustando o ruído ambiente' e 'Reconhecendo...', são exibidas no Log \"\n",
        "        \"de Mensagens.\"\n",
        "    )\n",
        "    messagebox.showinfo(\"Instruções\", instructions)\n",
        "\n",
        "# Função para adicionar mensagens de log na interface\n",
        "def log_message(message):\n",
        "    log_text.configure(state=tk.NORMAL)\n",
        "    log_text.insert(tk.END, message + '\\n')\n",
        "    log_text.configure(state=tk.DISABLED)\n",
        "    log_text.yview(tk.END)\n",
        "    root.update_idletasks()  # Atualiza a interface para garantir que o log seja exibido\n",
        "\n",
        "# Configuração da interface gráfica\n",
        "root = tk.Tk()\n",
        "root.title(\"DistilBert Question Answering\")\n",
        "root.geometry(\"800x600\")\n",
        "\n",
        "# Texto de introdução\n",
        "intro_label = tk.Label(root, text=\"BILL resposta contexto\")\n",
        "intro_label.pack(padx=10, pady=10)\n",
        "\n",
        "# Contexto\n",
        "context_label = tk.Label(root, text=\"Contexto:\")\n",
        "context_label.pack(anchor='w', padx=10)\n",
        "context_text = scrolledtext.ScrolledText(root, height=6, width=80)\n",
        "context_text.pack(padx=10, pady=5)\n",
        "context_text.insert(tk.END, \"\")\n",
        "\n",
        "# Pergunta\n",
        "question_label = tk.Label(root, text=\"Pergunta:\")\n",
        "question_label.pack(anchor='w', padx=10)\n",
        "question_entry = tk.Entry(root, width=80)\n",
        "question_entry.pack(padx=10, pady=5)\n",
        "\n",
        "# Botões \"Ativar Microfone\" e \"Instruções\"\n",
        "button_frame = tk.Frame(root)\n",
        "button_frame.pack(padx=10, pady=10)\n",
        "recognize_button = tk.Button(button_frame, text=\"Ativar Microfone\", command=on_recognize_button_click)\n",
        "recognize_button.grid(row=0, column=0, padx=5)\n",
        "instructions_button = tk.Button(button_frame, text=\"Instruções\", command=on_instructions_button_click)\n",
        "instructions_button.grid(row=0, column=1, padx=5)\n",
        "\n",
        "# Histórico de Respostas\n",
        "history_label = tk.Label(root, text=\"Histórico de Respostas:\")\n",
        "history_label.pack(anchor='w', padx=10)\n",
        "history_text = scrolledtext.ScrolledText(root, height=4, width=80, state=tk.DISABLED)\n",
        "history_text.pack(padx=10, pady=5)\n",
        "\n",
        "# Log de Mensagens\n",
        "log_label = tk.Label(root, text=\"Log de Mensagens:\")\n",
        "log_label.pack(anchor='w', padx=10)\n",
        "log_text = scrolledtext.ScrolledText(root, height=8, width=80, state=tk.DISABLED)\n",
        "log_text.pack(padx=10, pady=5)\n",
        "\n",
        "root.mainloop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voz com palavra-chave"
      ],
      "metadata": {
        "id": "8UXq56uEhAvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUBVoNEnXcE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
        "import torch\n",
        "import speech_recognition as sr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import playsound\n",
        "\n",
        "# Save question and answer in file\n",
        "def save_response(question, answer, file_name):\n",
        "    with open(file_name, 'a') as file:\n",
        "        file.write(f\"Question: {question}\\n\")\n",
        "        file.write(f\"Answer: {answer}\\n\")\n",
        "        file.write(\"\\n\")\n",
        "\n",
        "FILE_NAME = \"answer.txt\"\n",
        "\n",
        "# Load DistilBERT model and tokenizer\n",
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def generate_answer(context, question):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].squeeze().tolist()  # Convert tensor to list, removing singleton dimensions\n",
        "\n",
        "    # Get the scores of the answer positions\n",
        "    outputs = model(**inputs)\n",
        "    answer_start_scores = outputs.start_logits\n",
        "    answer_end_scores = outputs.end_logits\n",
        "\n",
        "    # Identify the start and end positions of the response\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    # Convert answer to text\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "\n",
        "    # Fix possible problems with spaces and tokens\n",
        "    answer = answer.strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "def wait_for_keyword(keywords, recognizer):\n",
        "    while True:\n",
        "        with sr.Microphone() as source:\n",
        "            print(\"Adjusting the ambient noise. Please wait...\")\n",
        "            recognizer.adjust_for_ambient_noise(source)\n",
        "            print(\"Waiting for Keyword...\")\n",
        "            audio_file = \"microphoneOn.mp3\"\n",
        "            playsound.playsound(audio_file)\n",
        "            audio = recognizer.listen(source)\n",
        "\n",
        "        try:\n",
        "            print(\"Recognizing...\")\n",
        "            text = recognizer.recognize_google(audio, language='en')\n",
        "            print(\"You said: \", text)\n",
        "\n",
        "            if any(keyword.lower() in text.lower() for keyword in keywords):\n",
        "                print(\"Keyword detected!\")\n",
        "                return True\n",
        "\n",
        "        except sr.UnknownValueError:\n",
        "            print(\"I don't understand what you said. Please, repeat.\")\n",
        "        except sr.RequestError as e:\n",
        "            print(f\"Error in the request to the Google API: {e}\")\n",
        "\n",
        "def recognize_speech_from_audio():\n",
        "    recognizer = sr.Recognizer()\n",
        "    with sr.Microphone() as source:\n",
        "        print(\"Adjusting the ambient noise. Please wait...\")\n",
        "        recognizer.adjust_for_ambient_noise(source)\n",
        "\n",
        "        print(\"Please ask your question...\")\n",
        "        audio_file = \"microphoneOn.mp3\"\n",
        "        playsound.playsound(audio_file)\n",
        "\n",
        "        audio = recognizer.listen(source)\n",
        "\n",
        "    try:\n",
        "        print(\"Recognizing...\")\n",
        "        return recognizer.recognize_google(audio, language=\"en\")\n",
        "    except sr.UnknownValueError:\n",
        "        return \"I don't understand you\"\n",
        "    except sr.RequestError:\n",
        "        return \"Error connecting to speech recognition service.\"\n",
        "\n",
        "def play_response(answer_text):\n",
        "    if answer_text.strip():\n",
        "        tts = gTTS(text=answer_text, lang=\"en\")\n",
        "        audio_file = \"answer.mp3\"\n",
        "        tts.save(audio_file)\n",
        "        playsound.playsound(audio_file)\n",
        "    else:\n",
        "        print(\"Empty response generated. Unable to play audio\")\n",
        "\n",
        "def main():\n",
        "    context = (\n",
        "    \"Tokyo is the capital city of Japan and one of the most populous urban areas in the world. It is known for its modern skyline, historic temples, and vibrant culture. Tokyo was originally a small fishing village named Edo. It became the seat of the Tokugawa shogunate in 1603, and the city was renamed Tokyo, which means 'Eastern Capital.' Today, Tokyo is famous for its bustling districts like Shibuya and Shinjuku, the historic Asakusa district, and landmarks such as the Tokyo Tower and the Imperial Palace. The city also boasts some of the best public transportation systems globally, including its extensive subway network. Tokyo is a hub for technology, fashion, and cuisine, offering a unique blend of traditional and contemporary experiences.\\n\\n\"\n",
        "    \"Economically, Tokyo is a major financial center, often ranking among the world's top cities for business and commerce. It is home to the Tokyo Stock Exchange, one of the largest stock exchanges in the world by market capitalization. The city's economy is diverse, encompassing sectors such as technology, manufacturing, and retail. Tokyo's Shibuya and Ginza districts are renowned for their high-end shopping and entertainment, drawing visitors from all over the globe.\\n\\n\"\n",
        "    \"Culturally, Tokyo is a melting pot of traditional Japanese culture and modern trends. It hosts numerous festivals throughout the year, including the cherry blossom festivals in Ueno Park and the lively Sumida River Fireworks Festival. The city is also known for its thriving arts scene, with many museums, galleries, and theaters. Akihabara, a district in Tokyo, is famous for its electronics shops and is a haven for anime and manga enthusiasts.\\n\\n\"\n",
        "    \"Tokyo is also a global leader in technology and innovation. The city is home to several high-tech industries and research institutions. It is known for its cutting-edge infrastructure, including the Shinkansen (bullet train) network that connects Tokyo with other major cities in Japan. Additionally, Tokyo's urban planning includes impressive skyscrapers, such as the Tokyo Skytree, which offers panoramic views of the city.\\n\\n\"\n",
        "    \"In terms of environment, Tokyo has made significant efforts to address sustainability. The city has various green spaces and parks, such as Yoyogi Park and Shinjuku Gyoen, where residents and visitors can enjoy nature amidst the urban landscape. Tokyo is also investing in eco-friendly technologies and practices to improve air quality and reduce its environmental footprint.\"\n",
        ")\n",
        "    print(\"Starting the QA system\")\n",
        "\n",
        "    while True:\n",
        "        question = recognize_speech_from_audio()\n",
        "        print(f\"Recognized Question: {question}\")\n",
        "\n",
        "        answer = generate_answer(context, question)\n",
        "        print(f\"Generated answer: {answer}\")\n",
        "\n",
        "        save_response(question, answer, FILE_NAME)\n",
        "\n",
        "        play_response(answer)\n",
        "\n",
        "def listen_microphone():\n",
        "    keywords = [\"Bill\", \"Hi Bill\", \"Ok Bill\"]\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    while True:\n",
        "        if wait_for_keyword(keywords, recognizer):\n",
        "            main()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    listen_microphone()\n"
      ],
      "metadata": {
        "id": "r17J6r9ThC8O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "d1feb61e-6f38-4571-d624-a3c0022329a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'speech_recognition'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-85922c066a42>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistilBertForQuestionAnswering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspeech_recognition\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgtts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgTTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'speech_recognition'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "k_oPd7gK5WXs"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}